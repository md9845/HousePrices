{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition for House Price predictions\n",
    "\n",
    "### Description from Kaggle\n",
    "#### Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n",
    "\n",
    "#### With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n",
    "\n",
    "#### https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "\n",
    "\n",
    "- This is a Supervised Learning Problem\n",
    "- The target variable is numeric\n",
    "- Source data is provided in csv files\n",
    "- Performance measure is Root Mean Squared Error (RMSE)\n",
    "- Lower RMSE is better\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following kernels have influenced my approach:\n",
    "- [Stacked Regressions : Top 4% on LeaderBoard](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard) by Serigne \n",
    "- [Kaggle Competition - House Prices: Advanced Regression Techniques](https://www.kaggle.com/hamzaben/tuned-random-forest-lasso-and-xgboost-regressors) by Hazma B\n",
    "- [Comprehensive data exploration with Python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python) by Pedro Marcelino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Kaggle competition for House Prices in Ames, Iowa.\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques'''\n",
    "   \n",
    "__author__ = 'Mike DiPalma'\n",
    "__email__ = 'mdipalma78@gmail.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from mlxtend.regressor import StackingCVRegressor, StackingRegressor\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, train_file, test_file, cat_cols, num_cols, target_col, id_col, trans_cols, group_cols, \\\n",
    "                 engineer_features, one_hot_encode):\n",
    "        '''create train and test dataframe'''\n",
    "        #create new copies instead of references\n",
    "        self.cat_cols = list(cat_cols)\n",
    "        self.num_cols = list(num_cols)\n",
    "        self.group_cols = list(group_cols)\n",
    "        self.feature_cols = cat_cols + num_cols\n",
    "        self.target_col = target_col\n",
    "        self.id_col = id_col\n",
    "        self.label_encoders = {}\n",
    "        self.train_df, self.test_df, self.test_Id, self.comb_df, self.feature_cols = self._create_df(train_file, test_file, cat_cols, \\\n",
    "                                                                                  num_cols, target_col, id_col, \\\n",
    "                                                                                  trans_cols, group_cols, \\\n",
    "                                                                                  engineer_features, one_hot_encode)\n",
    "\n",
    "    def _create_df(self, train_file, test_file, cat_cols, num_cols, target_col, id_col, trans_cols, group_cols, \\\n",
    "                   engineer_features=False, one_hot_encode=False):\n",
    "        '''loads training and test data, combines, preprocesses, encodes and seperates data into df for modeling'''\n",
    "        train_df = self._load_data(train_file)\n",
    "        test_df = self._load_data(test_file)\n",
    "        # Save the test ID columnn for the submission file\n",
    "        test_Id = test_df['Id']\n",
    "        train_df = self._clean_data(train_df)\n",
    "        train_df = self._shuffle_data(train_df)\n",
    "        comb_df = self._combine_data(train_df, test_df)\n",
    "        comb_df = self._transform_data_types(comb_df, trans_cols)\n",
    "        comb_df = self._fill_NaN(comb_df, num_cols, cat_cols)\n",
    "        print(\"Engineer Features: \", engineer_features)\n",
    "        if engineer_features:\n",
    "            comb_df = self.aggregate_features(comb_df)\n",
    "            comb_df = self.add_group_stats(comb_df)\n",
    "        print(\"One-hot Encode: \", one_hot_encode)\n",
    "        if one_hot_encode:\n",
    "            comb_df = self._one_hot_encode_df(comb_df, self.cat_cols, self.num_cols, target_col)\n",
    "        train_df = comb_df[comb_df['log1pSalePrice'].notnull()]\n",
    "        feature_cols = comb_df.drop(['log1pSalePrice'], axis=1).columns\n",
    "        test_df = comb_df[comb_df['log1pSalePrice'].isnull()]\n",
    "        test_df = test_df.drop(['log1pSalePrice'], axis=1)\n",
    "        return train_df, test_df, test_Id, comb_df, feature_cols\n",
    "\n",
    "    def _create_test_df(self, test_file, label_encode=True):\n",
    "        '''loads and label encodes test data'''\n",
    "        test_df = self._load_data(test_file)\n",
    "        if label_encode:\n",
    "            self.one_hot_encode_df(test_df, self.cat_cols)\n",
    "        return test_df\n",
    "        \n",
    "    def _load_data(self, file):\n",
    "        '''loads csv to pd dataframe'''\n",
    "        return pd.read_csv(file)\n",
    "    \n",
    "    def _combine_data(self, train, test):    \n",
    "        '''Joins train and test dataframes and resets the index'''\n",
    "        comb_df = pd.concat([train, test], sort=False)\n",
    "        comb_df = comb_df.reset_index(drop=True) # reset index\n",
    "        return comb_df\n",
    "\n",
    "    def _transform_data_types(self, df, cols):\n",
    "        '''Converts number fields which should be strings'''\n",
    "        for col in cols:\n",
    "            df[col] = df[col].apply(str)\n",
    "        return df\n",
    "    \n",
    "    def _fill_NaN(self, df, num_cols, cat_cols):\n",
    "        ''' replaces Nan values based on data type'''\n",
    "        for col in (num_cols):\n",
    "            df[col].fillna(0.0, inplace=True)\n",
    "        for col in (cat_cols):\n",
    "            df[col].fillna('None', inplace=True)\n",
    "        return df\n",
    "\n",
    "    def _one_hot_encode_df(self, df, cat_cols=None, num_cols=None, tar_col=None):\n",
    "        '''performs one-hot encoding on all categorical variables and combines result with continous variables'''\n",
    "        cat_df = pd.get_dummies(df[cat_cols], drop_first=True)\n",
    "        num_df = df[num_cols].apply(pd.to_numeric)\n",
    "        tar_df = df[tar_col].apply(pd.to_numeric)\n",
    "        return pd.concat([cat_df, num_df, tar_df], axis=1)#,ignore_index=False)\n",
    "   \n",
    "    def _clean_data(self, df):\n",
    "        '''removes outliers and applies log1p transformation to SalePrice'''\n",
    "        df = df.drop(df[(df['GrLivArea']>4000) & (df['SalePrice']<300000)].index)\n",
    "        df['log1pSalePrice'] = np.log1p(df['SalePrice'])\n",
    "        df = df.drop(columns=['SalePrice'])\n",
    "        return df\n",
    "\n",
    "    def _shuffle_data(self, df):\n",
    "        return shuffle(df).reset_index(drop=True)        \n",
    "    \n",
    "    def aggregate_features(self, df):\n",
    "        '''Combines the values of numeric features'''\n",
    "        df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "        df['TotalBath'] = df['FullBath'] + (0.5 * df['HalfBath']) + df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath'])\n",
    "        # update column lists\n",
    "        agg_cols = ['TotalSF', 'TotalBath']\n",
    "        self._extend_col_lists(num_cols=agg_cols)\n",
    "        return df\n",
    "\n",
    "    def _get_group_stats(self, df):\n",
    "        '''calculates group statistics'''\n",
    "        target_col = self.target_col\n",
    "        group_stats_df = pd.DataFrame({'group_mean': df.groupby(group_cols)[target_col].mean()})\n",
    "        group_stats_df['group_max'] = df.groupby(group_cols)[target_col].max()\n",
    "        group_stats_df['group_min'] = df.groupby(group_cols)[target_col].min()\n",
    "        group_stats_df['group_std'] = df.groupby(group_cols)[target_col].std()\n",
    "        group_stats_df['group_median'] = df.groupby(group_cols)[target_col].median()\n",
    "        group_stats_df.fillna(0, inplace=True)\n",
    "        return group_stats_df\n",
    "        \n",
    "    def add_group_stats(self, df):\n",
    "        '''adds group statistics to data stored in data object'''\n",
    "        #get group stats\n",
    "        group_stats_df = self._get_group_stats(df)\n",
    "        group_stats_df.reset_index(inplace=True)\n",
    "  \n",
    "        #merge derived columns to original df\n",
    "        df = self._merge_new_cols(df, group_stats_df, self.group_cols, fillna=False)\n",
    "        \n",
    "        #update column lists\n",
    "        group_stats_cols = ['group_mean', 'group_max', 'group_min', 'group_std', 'group_median']\n",
    "        self._extend_col_lists(num_cols=group_stats_cols)  \n",
    "        return df\n",
    "    \n",
    "    def _merge_new_cols(self, df, new_cols_df, keys, fillna=False):\n",
    "        '''merges engineered features with original df'''\n",
    "        df = pd.merge(df, new_cols_df, on=keys, how='left')\n",
    "        if fillna:\n",
    "            df.fillna(0, inplace=True)\n",
    "        return df\n",
    "        \n",
    "    def _extend_col_lists(self, cat_cols=[], num_cols=[]):\n",
    "        '''addes engineered feature cols to data col lists'''\n",
    "        self.num_cols.extend(num_cols)\n",
    "        self.cat_cols.extend(cat_cols)\n",
    "        self.feature_cols.extend(num_cols + cat_cols)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelContainer:\n",
    "    def __init__(self, models=[]):#, default_num_iters=10, verbose_lvl=0):\n",
    "        '''initializes model list and dicts'''\n",
    "        self.models = models\n",
    "        self.best_model = None\n",
    "        self.predictions = None\n",
    "        self.mean_rmse = {}\n",
    "        self.cv_std = {}\n",
    "        #self.default_num_iters = default_num_iters\n",
    "        #self.verbose_lvl = verbose_lvl\n",
    "        \n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "        print(model)\n",
    "\n",
    "    def cross_validate(self, data, k=3, num_procs=1):\n",
    "        '''cross validate models using given data'''\n",
    "        # set table to table to populate with performance results\n",
    "        rmse_results = []\n",
    "        names = []\n",
    "        col = ['Algorithm', 'RMSE Mean', 'RMSE SD']\n",
    "        results_df = pd.DataFrame(columns=col)\n",
    "        i = 0\n",
    "\n",
    "        feature_df = data.train_df[data.feature_cols]\n",
    "        target_df = data.train_df[data.target_col]\n",
    "        for name, model in self.models:\n",
    "            print(name, model)\n",
    "            print(\"Evaluating {}...\".format(name))\n",
    "            neg_mse = cross_val_score(model, feature_df, target_df, cv=k, n_jobs=num_procs, scoring='neg_mean_squared_error')\n",
    "            cv_rmse_results = np.sqrt(-neg_mse)\n",
    "            self.mean_rmse[model] = np.mean(cv_rmse_results)\n",
    "            self.cv_std[model] = np.std(cv_rmse_results)\n",
    "            rmse_results.append(cv_rmse_results)\n",
    "            names.append(name)\n",
    "            results_df.loc[i] = [name,\n",
    "                                 round(cv_rmse_results.mean(), 4),\n",
    "                                 round(cv_rmse_results.std(), 4)]\n",
    "            i += 1\n",
    "        results_df = results_df.sort_values(by=['RMSE Mean'], ascending=True).reset_index(drop=True)\n",
    "        print(results_df)\n",
    "    \n",
    "    def select_best_model(self):\n",
    "        '''select model with lowest mse'''\n",
    "        self.best_model = min(self.mean_rmse, key=self.mean_rmse.get)\n",
    "        \n",
    "    def best_model_fit(self, features, targets):\n",
    "        '''fits best model'''\n",
    "        self.best_model.fit(features, targets)\n",
    "    \n",
    "    def best_model_predict(self, features):\n",
    "        '''scores features using best model'''\n",
    "        self.predictions = self.best_model.predict(features)\n",
    "        self.predictions = np.expm1(self.predictions)\n",
    "\n",
    "    def blend_models_fit(self, features, targets):\n",
    "        '''fits models for blending'''\n",
    "        self.models[7][1].fit(features, targets)\n",
    "        self.models[6][1].fit(features, targets)\n",
    "        self.models[1][1].fit(features, targets)\n",
    "        self.models[3][1].fit(features, targets)\n",
    "        self.models[2][1].fit(features, targets)\n",
    "        self.models[5][1].fit(features, targets)\n",
    "    \n",
    "    def blend_models_predict(self, features):\n",
    "        self.predictions = ((0.4 * self.models[7][1].predict(features)) + # ensembleStack\\ \n",
    "            (0.3 * self.models[6][1].predict(features)) + # lightgbm\\ \n",
    "            (0.3 * self.models[1][1].predict(features)) + # ridge\\ \n",
    "            (0.0 * self.models[3][1].predict(features)) + # gbr\\ \n",
    "            (0.0 * self.models[2][1].predict(features))+ # rf\\ \n",
    "            (0.0 * self.models[5][1].predict(features)) # svr\n",
    "           )\n",
    "        self.predictions = np.expm1(self.predictions)\n",
    "    \n",
    "    def save_results(self, model, mean_mse, predictions):\n",
    "        '''saves model, model summary, feature importances, and predictions'''\n",
    "        with open('model.txt', 'w') as file:\n",
    "            file.write(str(model))\n",
    "        np.savetxt('predictions.csv', predictions, delimiter=',')\n",
    "        #create_submission(test_df, test_ID, predictions)\n",
    "   \n",
    "    @staticmethod\n",
    "    def get_feature_importance(model, cols):\n",
    "        '''retrieves and sorts feature importances'''\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            feature_importances = pd.DataFrame({'feature':cols, 'importance':importances})\n",
    "            feature_importances.sort_values(by='importance', ascending=False, inplace=True)\n",
    "            #set index to 'feature'\n",
    "            feature_importances.set_index('feature', inplace=True, drop=True)\n",
    "            return feature_importances[0:25]\n",
    "        else:\n",
    "            #some models don't have feature_importances_\n",
    "            return \"Feature importances do not exist for given model\"\n",
    "\n",
    "    def print_summary(self):\n",
    "        '''prints summary of models, best model, and feature importance'''\n",
    "        print('\\nModel Summaries:\\n')\n",
    "        for model in models.mean_rmse:\n",
    "            print('\\n', model, '\\n', 'RMSE:', models.mean_rmse[model])\n",
    "            print('\\n', 'Standard deviation during CV:\\n', models.cv_std[model])\n",
    "        print('\\nBest Model:\\n', models.best_model)\n",
    "        print('\\nMSE of Best Model\\n', models.mean_rmse[models.best_model])\n",
    "        print('\\nFeature Importances\\n', models.get_feature_importance(models.best_model, data.feature_cols))\n",
    "        #save results\n",
    "        print('Saving results.')\n",
    "        self.save_results(model, models.mean_rmse[model], self.predictions)\n",
    "        print('Creating submission file.')\n",
    "        self.create_submission(data.test_df, data.test_Id, self.predictions)\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importances = self.get_feature_importance(models.best_model, data.feature_cols)\n",
    "            feature_importances[0:25].plot.bar(figsize=(20,10))\n",
    "            plt.show()\n",
    "        else:\n",
    "            #some models don't have feature_importances_\n",
    "            return \"Feature importances do not exist for given model\"\n",
    "        \n",
    "    def create_submission(self, test_df, test_ID, predictions):\n",
    "        sub_df = pd.DataFrame({'Id': test_ID.values.tolist(), 'SalePrice': pd.Series(models.predictions, index=test_df.index)})\n",
    "\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\") \n",
    "\n",
    "        sub_df.to_csv('submission-' + timestr + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters needed to create and run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define number of CV folds\n",
    "k = 5\n",
    "\n",
    "# define number of processors to use for parallel runs\n",
    "num_procs = 4\n",
    "\n",
    "# set verbose level for models\n",
    "verbose_lvl = 0\n",
    "\n",
    "# Define inputs\n",
    "train_file = 'data/train.csv'\n",
    "test_file = 'data/test.csv'\n",
    "\n",
    "# Define variables\n",
    "cat_cols = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \\\n",
    "            'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', \\\n",
    "            'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', \\\n",
    "            'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \\\n",
    "            'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', \\\n",
    "            'Functional', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond', \\\n",
    "            'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition'] \n",
    "num_cols = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', \\\n",
    "            '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', \\\n",
    "            'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', \\\n",
    "            'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n",
    "target_col = 'log1pSalePrice'\n",
    "id_col = 'Id'\n",
    "transform_cols = ['MSSubClass', 'MoSold', 'YrSold', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', \\\n",
    "                  'OverallQual', 'OverallCond']\n",
    "group_cols = ['OverallQual']\n",
    "\n",
    "# Turn feature engineering on/off\n",
    "engineer_features = True\n",
    "\n",
    "# Turn one-hot encoding on/off\n",
    "one_hot_encode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineer Features:  True\n",
      "One-hot Encode:  True\n"
     ]
    }
   ],
   "source": [
    "data = Data(train_file, test_file, cat_cols, num_cols, target_col, id_col, transform_cols, group_cols, engineer_features, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model container and add models to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Linear Regression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False))\n",
      "('Ridge Regression', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=None, solver='auto', tol=0.001))\n",
      "('Random Forest', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=15,\n",
      "                      max_features=8, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=80,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=4,\n",
      "                      oob_score=False, random_state=None, verbose=0,\n",
      "                      warm_start=False))\n",
      "('Gradient Boosting Regressor', GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "                          learning_rate=0.1, loss='ls', max_depth=7,\n",
      "                          max_features=None, max_leaf_nodes=None,\n",
      "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                          min_samples_leaf=1, min_samples_split=2,\n",
      "                          min_weight_fraction_leaf=0.0, n_estimators=80,\n",
      "                          n_iter_no_change=None, presort='auto',\n",
      "                          random_state=None, subsample=1.0, tol=0.0001,\n",
      "                          validation_fraction=0.1, verbose=0, warm_start=False))\n",
      "('Stacking Regressor', StackingRegressor(meta_regressor=Ridge(alpha=1.0, copy_X=True,\n",
      "                                       fit_intercept=True, max_iter=None,\n",
      "                                       normalize=False, random_state=None,\n",
      "                                       solver='auto', tol=0.001),\n",
      "                  refit=True,\n",
      "                  regressors=(LinearRegression(copy_X=True, fit_intercept=True,\n",
      "                                               n_jobs=None, normalize=False),\n",
      "                              Ridge(alpha=1.0, copy_X=True, fit_intercept=True,\n",
      "                                    max_iter=None, normalize=False,\n",
      "                                    random_state=None, so...\n",
      "                                                        max_leaf_nodes=None,\n",
      "                                                        min_impurity_decrease=0.0,\n",
      "                                                        min_impurity_split=None,\n",
      "                                                        min_samples_leaf=1,\n",
      "                                                        min_samples_split=2,\n",
      "                                                        min_weight_fraction_leaf=0.0,\n",
      "                                                        n_estimators=80,\n",
      "                                                        n_iter_no_change=None,\n",
      "                                                        presort='auto',\n",
      "                                                        random_state=None,\n",
      "                                                        subsample=1.0,\n",
      "                                                        tol=0.0001,\n",
      "                                                        validation_fraction=0.1,\n",
      "                                                        verbose=0,\n",
      "                                                        warm_start=False)),\n",
      "                  store_train_meta_features=False,\n",
      "                  use_features_in_secondary=False, verbose=0))\n",
      "('Support Vector Regressor', SVR(C=20, cache_size=200, coef0=0.0, degree=3, epsilon=0.008, gamma=0.0003,\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False))\n",
      "('Light GBM', LGBMRegressor(bagging_fraction=0.8, bagging_freq=4, bagging_seed=8,\n",
      "              boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "              feature_fraction=0.2, feature_fraction_seed=8,\n",
      "              importance_type='split', learning_rate=0.01, max_bin=200,\n",
      "              max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n",
      "              min_split_gain=0.0, min_sum_hessian_in_leaf=11, n_estimators=7000,\n",
      "              n_jobs=-1, num_leaves=6, objective='regression', random_state=42,\n",
      "              reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "              subsample_for_bin=200000, subsample_freq=0, verbose=-1))\n"
     ]
    }
   ],
   "source": [
    "#create model container\n",
    "models = ModelContainer()\n",
    "\n",
    "#create models\n",
    "lr = LinearRegression()\n",
    "ridge = Ridge(alpha=1.0)\n",
    "rf = RandomForestRegressor(n_estimators=60, n_jobs=num_procs, max_depth=15, min_samples_split=80, \\\n",
    "                           max_features=8, verbose=verbose_lvl)\n",
    "gbr = GradientBoostingRegressor(n_estimators=80, max_depth=7, loss='ls', verbose=verbose_lvl)\n",
    "sr = StackingRegressor(regressors=(lr, ridge, rf, gbr), \\\n",
    "                       meta_regressor=ridge, \\\n",
    "                       use_features_in_secondary=False)\n",
    "svr = SVR(C= 20, epsilon= 0.008, gamma=0.0003)\n",
    "lightgbm = LGBMRegressor(objective='regression', \n",
    "                         num_leaves=6,\n",
    "                         learning_rate=0.01, \n",
    "                         n_estimators=7000,\n",
    "                         max_bin=200, \n",
    "                         bagging_fraction=0.8,\n",
    "                         bagging_freq=4, \n",
    "                         bagging_seed=8,\n",
    "                         feature_fraction=0.2,\n",
    "                         feature_fraction_seed=8,\n",
    "                         min_sum_hessian_in_leaf = 11,\n",
    "                         verbose=-1,\n",
    "                         random_state=42)\n",
    "\n",
    "\n",
    "#add models to model container\n",
    "models.add_model(('Linear Regression', lr))\n",
    "models.add_model(('Ridge Regression', ridge))\n",
    "models.add_model(('Random Forest', rf))\n",
    "models.add_model(('Gradient Boosting Regressor', gbr))\n",
    "models.add_model(('Stacking Regressor', sr))\n",
    "models.add_model(('Support Vector Regressor', svr))\n",
    "models.add_model(('Light GBM', lightgbm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ensemble stack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ensembleStack', StackingCVRegressor(cv=5,\n",
      "                    meta_regressor=Ridge(alpha=1.0, copy_X=True,\n",
      "                                         fit_intercept=True, max_iter=None,\n",
      "                                         normalize=False, random_state=None,\n",
      "                                         solver='auto', tol=0.001),\n",
      "                    n_jobs=None, pre_dispatch='2*n_jobs', random_state=None,\n",
      "                    refit=True,\n",
      "                    regressors=(LGBMRegressor(bagging_fraction=0.8,\n",
      "                                              bagging_freq=4, bagging_seed=8,\n",
      "                                              boosting_type='gbdt',\n",
      "                                              class_weight=None,\n",
      "                                              colsample...\n",
      "                                                          min_impurity_decrease=0.0,\n",
      "                                                          min_impurity_split=None,\n",
      "                                                          min_samples_leaf=1,\n",
      "                                                          min_samples_split=2,\n",
      "                                                          min_weight_fraction_leaf=0.0,\n",
      "                                                          n_estimators=80,\n",
      "                                                          n_iter_no_change=None,\n",
      "                                                          presort='auto',\n",
      "                                                          random_state=None,\n",
      "                                                          subsample=1.0,\n",
      "                                                          tol=0.0001,\n",
      "                                                          validation_fraction=0.1,\n",
      "                                                          verbose=0,\n",
      "                                                          warm_start=False)),\n",
      "                    shuffle=True, store_train_meta_features=False,\n",
      "                    use_features_in_secondary=False, verbose=0))\n"
     ]
    }
   ],
   "source": [
    "stack_gen = StackingCVRegressor(regressors=(lightgbm, ridge, rf, gbr), \\\n",
    "                                meta_regressor=ridge, \\\n",
    "                                use_features_in_secondary=False)\n",
    "\n",
    "#add to model container\n",
    "models.add_model(('ensembleStack', stack_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack_gen_model = stack_gen.fit(np.array(data.train_df[data.feature_cols]), np.array(data.train_df[data.target_col]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validate models, then select, fit, and score test data with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
      "Evaluating Linear Regression...\n",
      "Ridge Regression Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "Evaluating Ridge Regression...\n",
      "Random Forest RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=15,\n",
      "                      max_features=8, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=80,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=4,\n",
      "                      oob_score=False, random_state=None, verbose=0,\n",
      "                      warm_start=False)\n",
      "Evaluating Random Forest...\n",
      "Gradient Boosting Regressor GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "                          learning_rate=0.1, loss='ls', max_depth=7,\n",
      "                          max_features=None, max_leaf_nodes=None,\n",
      "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                          min_samples_leaf=1, min_samples_split=2,\n",
      "                          min_weight_fraction_leaf=0.0, n_estimators=80,\n",
      "                          n_iter_no_change=None, presort='auto',\n",
      "                          random_state=None, subsample=1.0, tol=0.0001,\n",
      "                          validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "Evaluating Gradient Boosting Regressor...\n",
      "Stacking Regressor StackingRegressor(meta_regressor=Ridge(alpha=1.0, copy_X=True,\n",
      "                                       fit_intercept=True, max_iter=None,\n",
      "                                       normalize=False, random_state=None,\n",
      "                                       solver='auto', tol=0.001),\n",
      "                  refit=True,\n",
      "                  regressors=(LinearRegression(copy_X=True, fit_intercept=True,\n",
      "                                               n_jobs=None, normalize=False),\n",
      "                              Ridge(alpha=1.0, copy_X=True, fit_intercept=True,\n",
      "                                    max_iter=None, normalize=False,\n",
      "                                    random_state=None, so...\n",
      "                                                        max_leaf_nodes=None,\n",
      "                                                        min_impurity_decrease=0.0,\n",
      "                                                        min_impurity_split=None,\n",
      "                                                        min_samples_leaf=1,\n",
      "                                                        min_samples_split=2,\n",
      "                                                        min_weight_fraction_leaf=0.0,\n",
      "                                                        n_estimators=80,\n",
      "                                                        n_iter_no_change=None,\n",
      "                                                        presort='auto',\n",
      "                                                        random_state=None,\n",
      "                                                        subsample=1.0,\n",
      "                                                        tol=0.0001,\n",
      "                                                        validation_fraction=0.1,\n",
      "                                                        verbose=0,\n",
      "                                                        warm_start=False)),\n",
      "                  store_train_meta_features=False,\n",
      "                  use_features_in_secondary=False, verbose=0)\n",
      "Evaluating Stacking Regressor...\n",
      "Support Vector Regressor SVR(C=20, cache_size=200, coef0=0.0, degree=3, epsilon=0.008, gamma=0.0003,\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
      "Evaluating Support Vector Regressor...\n",
      "Light GBM LGBMRegressor(bagging_fraction=0.8, bagging_freq=4, bagging_seed=8,\n",
      "              boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "              feature_fraction=0.2, feature_fraction_seed=8,\n",
      "              importance_type='split', learning_rate=0.01, max_bin=200,\n",
      "              max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n",
      "              min_split_gain=0.0, min_sum_hessian_in_leaf=11, n_estimators=7000,\n",
      "              n_jobs=-1, num_leaves=6, objective='regression', random_state=42,\n",
      "              reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "              subsample_for_bin=200000, subsample_freq=0, verbose=-1)\n",
      "Evaluating Light GBM...\n",
      "ensembleStack StackingCVRegressor(cv=5,\n",
      "                    meta_regressor=Ridge(alpha=1.0, copy_X=True,\n",
      "                                         fit_intercept=True, max_iter=None,\n",
      "                                         normalize=False, random_state=None,\n",
      "                                         solver='auto', tol=0.001),\n",
      "                    n_jobs=None, pre_dispatch='2*n_jobs', random_state=None,\n",
      "                    refit=True,\n",
      "                    regressors=(LGBMRegressor(bagging_fraction=0.8,\n",
      "                                              bagging_freq=4, bagging_seed=8,\n",
      "                                              boosting_type='gbdt',\n",
      "                                              class_weight=None,\n",
      "                                              colsample...\n",
      "                                                          min_impurity_decrease=0.0,\n",
      "                                                          min_impurity_split=None,\n",
      "                                                          min_samples_leaf=1,\n",
      "                                                          min_samples_split=2,\n",
      "                                                          min_weight_fraction_leaf=0.0,\n",
      "                                                          n_estimators=80,\n",
      "                                                          n_iter_no_change=None,\n",
      "                                                          presort='auto',\n",
      "                                                          random_state=None,\n",
      "                                                          subsample=1.0,\n",
      "                                                          tol=0.0001,\n",
      "                                                          validation_fraction=0.1,\n",
      "                                                          verbose=0,\n",
      "                                                          warm_start=False)),\n",
      "                    shuffle=True, store_train_meta_features=False,\n",
      "                    use_features_in_secondary=False, verbose=0)\n",
      "Evaluating ensembleStack...\n",
      "                     Algorithm  RMSE Mean   RMSE SD\n",
      "0                ensembleStack     0.1171    0.0069\n",
      "1                    Light GBM     0.1218    0.0063\n",
      "2             Ridge Regression     0.1287    0.0074\n",
      "3  Gradient Boosting Regressor     0.1364    0.0070\n",
      "4                Random Forest     0.1967    0.0080\n",
      "5     Support Vector Regressor     0.3971    0.0024\n",
      "6           Stacking Regressor    10.9377   21.6134\n",
      "7            Linear Regression    89.4604  178.5724\n"
     ]
    }
   ],
   "source": [
    "models.cross_validate(data, k, num_procs=num_procs)\n",
    "models.select_best_model()\n",
    "models.best_model_fit(data.train_df[data.feature_cols], data.train_df[data.target_col])\n",
    "models.best_model_predict(data.test_df[data.feature_cols])\n",
    "models.blend_models_fit(data.train_df[data.feature_cols], data.train_df[data.target_col])\n",
    "models.blend_models_predict(data.test_df[data.feature_cols])\n",
    "#models.save_results(models.best_model, models.predictions, models.feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summaries:\n",
      "\n",
      "\n",
      " LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) \n",
      " RMSE: 89.46044535815994\n",
      "\n",
      " Standard deviation during CV:\n",
      " 178.5723546645189\n",
      "\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=None, solver='auto', tol=0.001) \n",
      " RMSE: 0.12868992834162565\n",
      "\n",
      " Standard deviation during CV:\n",
      " 0.007402128244905619\n",
      "\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=15,\n",
      "                      max_features=8, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=80,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=4,\n",
      "                      oob_score=False, random_state=None, verbose=0,\n",
      "                      warm_start=False) \n",
      " RMSE: 0.19668246922772828\n",
      "\n",
      " Standard deviation during CV:\n",
      " 0.007981630088831277\n",
      "\n",
      " GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "                          learning_rate=0.1, loss='ls', max_depth=7,\n",
      "                          max_features=None, max_leaf_nodes=None,\n",
      "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                          min_samples_leaf=1, min_samples_split=2,\n",
      "                          min_weight_fraction_leaf=0.0, n_estimators=80,\n",
      "                          n_iter_no_change=None, presort='auto',\n",
      "                          random_state=None, subsample=1.0, tol=0.0001,\n",
      "                          validation_fraction=0.1, verbose=0, warm_start=False) \n",
      " RMSE: 0.13643888351828498\n",
      "\n",
      " Standard deviation during CV:\n",
      " 0.007015687346092609\n",
      "\n",
      " StackingRegressor(meta_regressor=Ridge(alpha=1.0, copy_X=True,\n",
      "                                       fit_intercept=True, max_iter=None,\n",
      "                                       normalize=False, random_state=None,\n",
      "                                       solver='auto', tol=0.001),\n",
      "                  refit=True,\n",
      "                  regressors=(LinearRegression(copy_X=True, fit_intercept=True,\n",
      "                                               n_jobs=None, normalize=False),\n",
      "                              Ridge(alpha=1.0, copy_X=True, fit_intercept=True,\n",
      "                                    max_iter=None, normalize=False,\n",
      "                                    random_state=None, so...\n",
      "                                                        max_leaf_nodes=None,\n",
      "                                                        min_impurity_decrease=0.0,\n",
      "                                                        min_impurity_split=None,\n",
      "                                                        min_samples_leaf=1,\n",
      "                                                        min_samples_split=2,\n",
      "                                                        min_weight_fraction_leaf=0.0,\n",
      "                                                        n_estimators=80,\n",
      "                                                        n_iter_no_change=None,\n",
      "                                                        presort='auto',\n",
      "                                                        random_state=None,\n",
      "                                                        subsample=1.0,\n",
      "                                                        tol=0.0001,\n",
      "                                                        validation_fraction=0.1,\n",
      "                                                        verbose=0,\n",
      "                                                        warm_start=False)),\n",
      "                  store_train_meta_features=False,\n",
      "                  use_features_in_secondary=False, verbose=0) \n",
      " RMSE: 10.937742676991341\n",
      "\n",
      " Standard deviation during CV:\n",
      " 21.613428462137414\n",
      "\n",
      " SVR(C=20, cache_size=200, coef0=0.0, degree=3, epsilon=0.008, gamma=0.0003,\n",
      "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False) \n",
      " RMSE: 0.39714751636011275\n",
      "\n",
      " Standard deviation during CV:\n",
      " 0.0023869485477790425\n",
      "\n",
      " LGBMRegressor(bagging_fraction=0.8, bagging_freq=4, bagging_seed=8,\n",
      "              boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "              feature_fraction=0.2, feature_fraction_seed=8,\n",
      "              importance_type='split', learning_rate=0.01, max_bin=200,\n",
      "              max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n",
      "              min_split_gain=0.0, min_sum_hessian_in_leaf=11, n_estimators=7000,\n",
      "              n_jobs=-1, num_leaves=6, objective='regression', random_state=42,\n",
      "              reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
      "              subsample_for_bin=200000, subsample_freq=0, verbose=-1) \n",
      " RMSE: 0.12177587049475103\n",
      "\n",
      " Standard deviation during CV:\n",
      " 0.006342951836139249\n",
      "\n",
      " StackingCVRegressor(cv=5,\n",
      "                    meta_regressor=Ridge(alpha=1.0, copy_X=True,\n",
      "                                         fit_intercept=True, max_iter=None,\n",
      "                                         normalize=False, random_state=None,\n",
      "                                         solver='auto', tol=0.001),\n",
      "                    n_jobs=None, pre_dispatch='2*n_jobs', random_state=None,\n",
      "                    refit=True,\n",
      "                    regressors=(LGBMRegressor(bagging_fraction=0.8,\n",
      "                                              bagging_freq=4, bagging_seed=8,\n",
      "                                              boosting_type='gbdt',\n",
      "                                              class_weight=None,\n",
      "                                              colsample...\n",
      "                                                          min_impurity_decrease=0.0,\n",
      "                                                          min_impurity_split=None,\n",
      "                                                          min_samples_leaf=1,\n",
      "                                                          min_samples_split=2,\n",
      "                                                          min_weight_fraction_leaf=0.0,\n",
      "                                                          n_estimators=80,\n",
      "                                                          n_iter_no_change=None,\n",
      "                                                          presort='auto',\n",
      "                                                          random_state=None,\n",
      "                                                          subsample=1.0,\n",
      "                                                          tol=0.0001,\n",
      "                                                          validation_fraction=0.1,\n",
      "                                                          verbose=0,\n",
      "                                                          warm_start=False)),\n",
      "                    shuffle=True, store_train_meta_features=False,\n",
      "                    use_features_in_secondary=False, verbose=0) \n",
      " RMSE: 0.11713534709959847\n",
      "\n",
      " Standard deviation during CV:\n",
      " 0.006912838035037971\n",
      "\n",
      "Best Model:\n",
      " StackingCVRegressor(cv=5,\n",
      "                    meta_regressor=Ridge(alpha=1.0, copy_X=True,\n",
      "                                         fit_intercept=True, max_iter=None,\n",
      "                                         normalize=False, random_state=None,\n",
      "                                         solver='auto', tol=0.001),\n",
      "                    n_jobs=None, pre_dispatch='2*n_jobs', random_state=None,\n",
      "                    refit=True,\n",
      "                    regressors=(LGBMRegressor(bagging_fraction=0.8,\n",
      "                                              bagging_freq=4, bagging_seed=8,\n",
      "                                              boosting_type='gbdt',\n",
      "                                              class_weight=None,\n",
      "                                              colsample...\n",
      "                                                          min_impurity_decrease=0.0,\n",
      "                                                          min_impurity_split=None,\n",
      "                                                          min_samples_leaf=1,\n",
      "                                                          min_samples_split=2,\n",
      "                                                          min_weight_fraction_leaf=0.0,\n",
      "                                                          n_estimators=80,\n",
      "                                                          n_iter_no_change=None,\n",
      "                                                          presort='auto',\n",
      "                                                          random_state=None,\n",
      "                                                          subsample=1.0,\n",
      "                                                          tol=0.0001,\n",
      "                                                          validation_fraction=0.1,\n",
      "                                                          verbose=0,\n",
      "                                                          warm_start=False)),\n",
      "                    shuffle=True, store_train_meta_features=False,\n",
      "                    use_features_in_secondary=False, verbose=0)\n",
      "\n",
      "MSE of Best Model\n",
      " 0.11713534709959847\n",
      "\n",
      "Feature Importances\n",
      " Feature importances do not exist for given model\n",
      "Saving results.\n",
      "Creating submission file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Feature importances do not exist for given model'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
