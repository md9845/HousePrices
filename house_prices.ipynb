{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Kaggle competition for House Prices in Ames, Iowa.\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques'''\n",
    "   \n",
    "__author__ = 'Mike DiPalma'\n",
    "__email__ = 'mdipalma78@gmail.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlxtend.regressor import StackingCVRegressor, StackingRegressor\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, train_file, test_file, cat_cols, num_cols, target_col, id_col, trans_cols, group_cols, \\\n",
    "                 engineer_features, one_hot_encode):\n",
    "        '''create train and test dataframe'''\n",
    "        #create new copies instead of references\n",
    "        self.cat_cols = list(cat_cols)\n",
    "        self.num_cols = list(num_cols)\n",
    "        self.group_cols = list(group_cols)\n",
    "        self.feature_cols = cat_cols + num_cols\n",
    "        self.target_col = target_col\n",
    "        self.id_col = id_col\n",
    "        self.label_encoders = {}\n",
    "        self.train_df, self.test_df, self.test_Id, self.comb_df, self.feature_cols = self._create_df(train_file, test_file, cat_cols, \\\n",
    "                                                                                  num_cols, target_col, id_col, \\\n",
    "                                                                                  trans_cols, group_cols, \\\n",
    "                                                                                  engineer_features, one_hot_encode)\n",
    "\n",
    "    def _create_df(self, train_file, test_file, cat_cols, num_cols, target_col, id_col, trans_cols, group_cols, \\\n",
    "                   engineer_features=False, one_hot_encode=False):\n",
    "        '''loads training and test data, combines, preprocesses, encodes and seperates data into df for modeling'''\n",
    "        train_df = self._load_data(train_file)\n",
    "        test_df = self._load_data(test_file)\n",
    "        # Save the test ID columnn for the submission file\n",
    "        test_Id = test_df['Id']\n",
    "        train_df = self._clean_data(train_df)\n",
    "        train_df = self._shuffle_data(train_df)\n",
    "        comb_df = self._combine_data(train_df, test_df)\n",
    "        comb_df = self._transform_data_types(comb_df, trans_cols)\n",
    "        comb_df = self._fill_NaN(comb_df, num_cols, cat_cols)\n",
    "        print(\"Engineer Features: \", engineer_features)\n",
    "        if engineer_features:\n",
    "            comb_df = self.aggregate_features(comb_df)\n",
    "            comb_df = self.add_group_stats(comb_df)\n",
    "        print(\"One-hot Encode: \", one_hot_encode)\n",
    "        if one_hot_encode:\n",
    "            comb_df = self._one_hot_encode_df(comb_df, self.cat_cols, self.num_cols, target_col)\n",
    "        train_df = comb_df[comb_df['log1pSalePrice'].notnull()]\n",
    "        feature_cols = comb_df.drop(['log1pSalePrice'], axis=1).columns\n",
    "        test_df = comb_df[comb_df['log1pSalePrice'].isnull()]\n",
    "        test_df = test_df.drop(['log1pSalePrice'], axis=1)\n",
    "        return train_df, test_df, test_Id, comb_df, feature_cols\n",
    "\n",
    "    def _create_test_df(self, test_file, label_encode=True):\n",
    "        '''loads and label encodes test data'''\n",
    "        test_df = self._load_data(test_file)\n",
    "        if label_encode:\n",
    "            self.one_hot_encode_df(test_df, self.cat_cols)\n",
    "        return test_df\n",
    "        \n",
    "    def _load_data(self, file):\n",
    "        '''loads csv to pd dataframe'''\n",
    "        return pd.read_csv(file)\n",
    "    \n",
    "    def _combine_data(self, train, test):    \n",
    "        '''Joins train and test dataframes and resets the index'''\n",
    "        comb_df = pd.concat([train, test], sort=False)\n",
    "        comb_df = comb_df.reset_index(drop=True) # reset index\n",
    "        return comb_df\n",
    "\n",
    "    def _transform_data_types(self, df, cols):\n",
    "        '''Converts number fields which should be strings'''\n",
    "        for col in cols:\n",
    "            df[col] = df[col].apply(str)\n",
    "        return df\n",
    "    \n",
    "    def _fill_NaN(self, df, num_cols, cat_cols):\n",
    "        ''' replaces Nan values based on data type'''\n",
    "        for col in (num_cols):\n",
    "            df[col].fillna(0.0, inplace=True)\n",
    "        for col in (cat_cols):\n",
    "            df[col].fillna('None', inplace=True)\n",
    "        return df\n",
    "\n",
    "    def _one_hot_encode_df(self, df, cat_cols=None, num_cols=None, tar_col=None):\n",
    "        '''performs one-hot encoding on all categorical variables and combines result with continous variables'''\n",
    "        cat_df = pd.get_dummies(df[cat_cols], drop_first=True)\n",
    "        num_df = df[num_cols].apply(pd.to_numeric)\n",
    "        tar_df = df[tar_col].apply(pd.to_numeric)\n",
    "        return pd.concat([cat_df, num_df, tar_df], axis=1)#,ignore_index=False)\n",
    "   \n",
    "    def _clean_data(self, df):\n",
    "        '''removes outliers and applies log1p transformation to SalePrice'''\n",
    "        df = df.drop(df[(df['GrLivArea']>4000) & (df['SalePrice']<300000)].index)\n",
    "        df['log1pSalePrice'] = np.log1p(df['SalePrice'])\n",
    "        df = df.drop(columns=['SalePrice'])\n",
    "        return df\n",
    "\n",
    "    def _shuffle_data(self, df):\n",
    "        return shuffle(df).reset_index(drop=True)        \n",
    "    \n",
    "    def aggregate_features(self, df):\n",
    "        '''Combines the values of numeric features'''\n",
    "        df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "        df['TotalBath'] = df['FullBath'] + (0.5 * df['HalfBath']) + df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath'])\n",
    "        # update column lists\n",
    "        agg_cols = ['TotalSF', 'TotalBath']\n",
    "        self._extend_col_lists(num_cols=agg_cols)\n",
    "        return df\n",
    "\n",
    "    def _get_group_stats(self, df):\n",
    "        '''calculates group statistics'''\n",
    "        target_col = self.target_col\n",
    "        group_stats_df = pd.DataFrame({'group_mean': df.groupby(group_cols)[target_col].mean()})\n",
    "        group_stats_df['group_max'] = df.groupby(group_cols)[target_col].max()\n",
    "        group_stats_df['group_min'] = df.groupby(group_cols)[target_col].min()\n",
    "        group_stats_df['group_std'] = df.groupby(group_cols)[target_col].std()\n",
    "        group_stats_df['group_median'] = df.groupby(group_cols)[target_col].median()\n",
    "        group_stats_df.fillna(0, inplace=True)\n",
    "        return group_stats_df\n",
    "        \n",
    "    def add_group_stats(self, df):\n",
    "        '''adds group statistics to data stored in data object'''\n",
    "        #get group stats\n",
    "        group_stats_df = self._get_group_stats(df)\n",
    "        group_stats_df.reset_index(inplace=True)\n",
    "  \n",
    "        #merge derived columns to original df\n",
    "        df = self._merge_new_cols(df, group_stats_df, self.group_cols, fillna=False)\n",
    "        \n",
    "        #update column lists\n",
    "        group_stats_cols = ['group_mean', 'group_max', 'group_min', 'group_std', 'group_median']\n",
    "        self._extend_col_lists(num_cols=group_stats_cols)  \n",
    "        return df\n",
    "    \n",
    "    def _merge_new_cols(self, df, new_cols_df, keys, fillna=False):\n",
    "        '''merges engineered features with original df'''\n",
    "        df = pd.merge(df, new_cols_df, on=keys, how='left')\n",
    "        if fillna:\n",
    "            df.fillna(0, inplace=True)\n",
    "        return df\n",
    "        \n",
    "    def _extend_col_lists(self, cat_cols=[], num_cols=[]):\n",
    "        '''addes engineered feature cols to data col lists'''\n",
    "        self.num_cols.extend(num_cols)\n",
    "        self.cat_cols.extend(cat_cols)\n",
    "        self.feature_cols.extend(num_cols + cat_cols)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelContainer:\n",
    "    def __init__(self, models=[]):#, default_num_iters=10, verbose_lvl=0):\n",
    "        '''initializes model list and dicts'''\n",
    "        self.models = models\n",
    "        self.best_model = None\n",
    "        self.predictions = None\n",
    "        self.mean_rmse = {}\n",
    "        self.cv_std = {}\n",
    "        #self.default_num_iters = default_num_iters\n",
    "        #self.verbose_lvl = verbose_lvl\n",
    "        \n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "        print(model)\n",
    "\n",
    "    def cross_validate(self, data, k=3, num_procs=1):\n",
    "        '''cross validate models using given data'''\n",
    "        feature_df = data.train_df[data.feature_cols]\n",
    "        target_df = data.train_df[data.target_col]\n",
    "        for model in self.models:\n",
    "            print(model)\n",
    "            neg_mse = cross_val_score(model, feature_df, target_df, cv=k, n_jobs=num_procs, scoring='neg_mean_squared_error')\n",
    "            cv_rmse_results = np.sqrt(-neg_mse)\n",
    "            self.mean_rmse[model] = np.mean(cv_rmse_results)\n",
    "            self.cv_std[model] = np.std(cv_rmse_results)    \n",
    "    \n",
    "    def select_best_model(self):\n",
    "        '''select model with lowest mse'''\n",
    "        self.best_model = min(self.mean_rmse, key=self.mean_rmse.get)\n",
    "        \n",
    "    def best_model_fit(self, features, targets):\n",
    "        '''fits best model'''\n",
    "        self.best_model.fit(features, targets)\n",
    "    \n",
    "    def best_model_predict(self, features):\n",
    "        '''scores features using best model'''\n",
    "        self.predictions = self.best_model.predict(features)\n",
    "        self.predictions = np.expm1(self.predictions)\n",
    "\n",
    "        \n",
    "    def save_results(self, model, mean_mse, predictions):\n",
    "        '''saves model, model summary, feature importances, and predictions'''\n",
    "        with open('model.txt', 'w') as file:\n",
    "            file.write(str(model))\n",
    "        np.savetxt('predictions.csv', predictions, delimiter=',')\n",
    "        #create_submission(test_df, test_ID, predictions)\n",
    "   \n",
    "    @staticmethod\n",
    "    def get_feature_importance(model, cols):\n",
    "        '''retrieves and sorts feature importances'''\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            feature_importances = pd.DataFrame({'feature':cols, 'importance':importances})\n",
    "            feature_importances.sort_values(by='importance', ascending=False, inplace=True)\n",
    "            #set index to 'feature'\n",
    "            feature_importances.set_index('feature', inplace=True, drop=True)\n",
    "            return feature_importances[0:25]\n",
    "        else:\n",
    "            #some models don't have feature_importances_\n",
    "            return \"Feature importances do not exist for given model\"\n",
    "\n",
    "    def print_summary(self):\n",
    "        '''prints summary of models, best model, and feature importance'''\n",
    "        print('\\nModel Summaries:\\n')\n",
    "        for model in models.mean_rmse:\n",
    "            print('\\n', model, '\\n', 'RMSE:', models.mean_rmse[model])\n",
    "            print('\\n', 'Standard deviation during CV:\\n', models.cv_std[model])\n",
    "        print('\\nBest Model:\\n', models.best_model)\n",
    "        print('\\nMSE of Best Model\\n', models.mean_rmse[models.best_model])\n",
    "        print('\\nFeature Importances\\n', models.get_feature_importance(models.best_model, data.feature_cols))\n",
    "        #save results\n",
    "        print('Saving results.')\n",
    "        self.save_results(model, models.mean_rmse[model], self.predictions)\n",
    "        print('Creating submission file.')\n",
    "        self.create_submission(data.test_df, data.test_Id, self.predictions)\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importances = self.get_feature_importance(models.best_model, data.feature_cols)\n",
    "            feature_importances[0:25].plot.bar(figsize=(20,10))\n",
    "            plt.show()\n",
    "        else:\n",
    "            #some models don't have feature_importances_\n",
    "            return \"Feature importances do not exist for given model\"\n",
    "        \n",
    "    def create_submission(self, test_df, test_ID, predictions):\n",
    "        sub_df = pd.DataFrame({'Id': test_ID.values.tolist(), 'SalePrice': pd.Series(models.predictions, index=test_df.index)})\n",
    "\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\") \n",
    "\n",
    "        sub_df.to_csv('submission-' + timestr + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters needed to create and run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define number of processors to use for parallel runs\n",
    "num_procs = 4\n",
    "\n",
    "# set verbose level for models\n",
    "verbose_lvl = 0\n",
    "\n",
    "# Define inputs\n",
    "train_file = 'data/train.csv'\n",
    "test_file = 'data/test.csv'\n",
    "\n",
    "# Define variables\n",
    "cat_cols = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \\\n",
    "            'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', \\\n",
    "            'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', \\\n",
    "            'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \\\n",
    "            'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', \\\n",
    "            'Functional', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond', \\\n",
    "            'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition'] \n",
    "num_cols = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', \\\n",
    "            '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', \\\n",
    "            'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', \\\n",
    "            'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n",
    "target_col = 'log1pSalePrice'\n",
    "id_col = 'Id'\n",
    "transform_cols = ['MSSubClass', 'MoSold', 'YrSold', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', \\\n",
    "                  'OverallQual', 'OverallCond']\n",
    "group_cols = ['OverallQual']\n",
    "#group_cols = ['OverallQual', 'Neighborhood', 'YearBuilt', 'ExterQual', 'BsmtQual', 'GarageYrBlt', 'KitchenQual', \\\n",
    "#              'GarageFinish', 'YearRemodAdd', 'GarageType']\n",
    "# Turn feature engineering on/off\n",
    "engineer_features = True\n",
    "\n",
    "# Turn one-hot encoding on/off\n",
    "one_hot_encode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineer Features:  True\n",
      "One-hot Encode:  True\n"
     ]
    }
   ],
   "source": [
    "data = Data(train_file, test_file, cat_cols, num_cols, target_col, id_col, transform_cols, group_cols, engineer_features, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model container and add models to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=15,\n",
      "                      max_features=8, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=80,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=4,\n",
      "                      oob_score=False, random_state=None, verbose=0,\n",
      "                      warm_start=False)\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "                          learning_rate=0.1, loss='ls', max_depth=7,\n",
      "                          max_features=None, max_leaf_nodes=None,\n",
      "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                          min_samples_leaf=1, min_samples_split=2,\n",
      "                          min_weight_fraction_leaf=0.0, n_estimators=80,\n",
      "                          n_iter_no_change=None, presort='auto',\n",
      "                          random_state=None, subsample=1.0, tol=0.0001,\n",
      "                          validation_fraction=0.1, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "#create model container\n",
    "models = ModelContainer()\n",
    "\n",
    "#create models -- hyperparameter tuning already done by hand for each model\n",
    "models.add_model(LinearRegression())\n",
    "models.add_model(Ridge(alpha=1.0))\n",
    "models.add_model(RandomForestRegressor(n_estimators=60, n_jobs=num_procs, max_depth=15, min_samples_split=80, \\\n",
    "                                       max_features=8, verbose=verbose_lvl))\n",
    "models.add_model(GradientBoostingRegressor(n_estimators=80, max_depth=7, loss='ls', verbose=verbose_lvl))\n",
    "#models.add_model(StackingRegressor(regressors=(LinearRegression, Ridge, RandomForestRegressor, \\\n",
    "#                                                                    GradientBoostingRegressor), \\\n",
    "#                                                        meta_regressor=Ridge, \\\n",
    "#                                                        use_features_in_secondary=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ensemble stack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_gen = StackingCVRegressor(regressors=(LinearRegression, Ridge, RandomForestRegressor, GradientBoostingRegressor), \\\n",
    "                                meta_regressor=Ridge, \\\n",
    "                                use_features_in_secondary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_gen_model = stack_gen.fit(np.array(df_train_ml), np.array(target_ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validate models, then select, fit, and score test data with best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
      "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=15,\n",
      "                      max_features=8, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=80,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=4,\n",
      "                      oob_score=False, random_state=None, verbose=0,\n",
      "                      warm_start=False)\n",
      "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "                          learning_rate=0.1, loss='ls', max_depth=7,\n",
      "                          max_features=None, max_leaf_nodes=None,\n",
      "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                          min_samples_leaf=1, min_samples_split=2,\n",
      "                          min_weight_fraction_leaf=0.0, n_estimators=80,\n",
      "                          n_iter_no_change=None, presort='auto',\n",
      "                          random_state=None, subsample=1.0, tol=0.0001,\n",
      "                          validation_fraction=0.1, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "models.cross_validate(data, k=2, num_procs=num_procs)\n",
    "models.select_best_model()\n",
    "models.best_model_fit(data.train_df[data.feature_cols], data.train_df[data.target_col])\n",
    "models.best_model_predict(data.test_df[data.feature_cols])\n",
    "#models.save_results(models.best_model, models.predictions, models.feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summaries:\n",
      "\n",
      "\n",
      " LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) \n",
      " RMSE: 0.2753442822125305\n",
      "\n",
      " Standard deviation during CV:\n",
      " 0.018923476599355143\n",
      "\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=None, solver='auto', tol=0.001) \n",
      " RMSE: 0.14141924858549892\n",
      "\n",
      " Standard deviation during CV:\n",
      " 0.0020921214838325874\n",
      "\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=15,\n",
      "                      max_features=8, max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=1, min_samples_split=80,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=4,\n",
      "                      oob_score=False, random_state=None, verbose=0,\n",
      "                      warm_start=False) \n",
      " RMSE: 0.21333008668270853\n",
      "\n",
      " Standard deviation during CV:\n",
      " 0.013442919507907505\n",
      "\n",
      " GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "                          learning_rate=0.1, loss='ls', max_depth=7,\n",
      "                          max_features=None, max_leaf_nodes=None,\n",
      "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                          min_samples_leaf=1, min_samples_split=2,\n",
      "                          min_weight_fraction_leaf=0.0, n_estimators=80,\n",
      "                          n_iter_no_change=None, presort='auto',\n",
      "                          random_state=None, subsample=1.0, tol=0.0001,\n",
      "                          validation_fraction=0.1, verbose=0, warm_start=False) \n",
      " RMSE: 0.1480080989222069\n",
      "\n",
      " Standard deviation during CV:\n",
      " 0.006821356835529993\n",
      "\n",
      "Best Model:\n",
      " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "      normalize=False, random_state=None, solver='auto', tol=0.001)\n",
      "\n",
      "MSE of Best Model\n",
      " 0.14141924858549892\n",
      "\n",
      "Feature Importances\n",
      " Feature importances do not exist for given model\n",
      "Saving results.\n",
      "Creating submission file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Feature importances do not exist for given model'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.print_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
